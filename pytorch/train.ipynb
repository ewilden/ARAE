{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from utils import to_gpu, Corpus, batchify, train_ngram_lm, get_ppl\n",
    "from models import Seq2Seq, MLP_D, MLP_G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] --data_path DATA_PATH\n",
      "                             [--kenlm_path KENLM_PATH] [--outf OUTF]\n",
      "                             [--vocab_size VOCAB_SIZE] [--maxlen MAXLEN]\n",
      "                             [--lowercase] [--emsize EMSIZE]\n",
      "                             [--nhidden NHIDDEN] [--nlayers NLAYERS]\n",
      "                             [--noise_radius NOISE_RADIUS]\n",
      "                             [--noise_anneal NOISE_ANNEAL] [--hidden_init]\n",
      "                             [--arch_g ARCH_G] [--arch_d ARCH_D]\n",
      "                             [--z_size Z_SIZE] [--temp TEMP]\n",
      "                             [--enc_grad_norm ENC_GRAD_NORM]\n",
      "                             [--gan_toenc GAN_TOENC] [--dropout DROPOUT]\n",
      "                             [--epochs EPOCHS] [--min_epochs MIN_EPOCHS]\n",
      "                             [--no_earlystopping] [--patience PATIENCE]\n",
      "                             [--batch_size N] [--niters_ae NITERS_AE]\n",
      "                             [--niters_gan_d NITERS_GAN_D]\n",
      "                             [--niters_gan_g NITERS_GAN_G]\n",
      "                             [--niters_gan_schedule NITERS_GAN_SCHEDULE]\n",
      "                             [--lr_ae LR_AE] [--lr_gan_g LR_GAN_G]\n",
      "                             [--lr_gan_d LR_GAN_D] [--beta1 BETA1]\n",
      "                             [--clip CLIP] [--gan_clamp GAN_CLAMP] [--sample]\n",
      "                             [--N N] [--log_interval LOG_INTERVAL]\n",
      "                             [--seed SEED] [--cuda]\n",
      "ipykernel_launcher.py: error: the following arguments are required: --data_path\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2918: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='PyTorch ARAE for Text')\n",
    "# Path Arguments\n",
    "parser.add_argument('--data_path', type=str, required=True,\n",
    "                    help='location of the data corpus')\n",
    "parser.add_argument('--kenlm_path', type=str, default='../Data/kenlm',\n",
    "                    help='path to kenlm directory')\n",
    "parser.add_argument('--outf', type=str, default='example',\n",
    "                    help='output directory name')\n",
    "\n",
    "# Data Processing Arguments\n",
    "parser.add_argument('--vocab_size', type=int, default=11000,\n",
    "                    help='cut vocabulary down to this size '\n",
    "                         '(most frequently seen words in train)')\n",
    "parser.add_argument('--maxlen', type=int, default=30,\n",
    "                    help='maximum sentence length')\n",
    "parser.add_argument('--lowercase', action='store_true',\n",
    "                    help='lowercase all text')\n",
    "\n",
    "# Model Arguments\n",
    "parser.add_argument('--emsize', type=int, default=300,\n",
    "                    help='size of word embeddings')\n",
    "parser.add_argument('--nhidden', type=int, default=300,\n",
    "                    help='number of hidden units per layer')\n",
    "parser.add_argument('--nlayers', type=int, default=1,\n",
    "                    help='number of layers')\n",
    "parser.add_argument('--noise_radius', type=float, default=0.2,\n",
    "                    help='stdev of noise for autoencoder (regularizer)')\n",
    "parser.add_argument('--noise_anneal', type=float, default=0.995,\n",
    "                    help='anneal noise_radius exponentially by this'\n",
    "                         'every 100 iterations')\n",
    "parser.add_argument('--hidden_init', action='store_true',\n",
    "                    help=\"initialize decoder hidden state with encoder's\")\n",
    "parser.add_argument('--arch_g', type=str, default='300-300',\n",
    "                    help='generator architecture (MLP)')\n",
    "parser.add_argument('--arch_d', type=str, default='300-300',\n",
    "                    help='critic/discriminator architecture (MLP)')\n",
    "parser.add_argument('--z_size', type=int, default=100,\n",
    "                    help='dimension of random noise z to feed into generator')\n",
    "parser.add_argument('--temp', type=float, default=1,\n",
    "                    help='softmax temperature (lower --> more discrete)')\n",
    "parser.add_argument('--enc_grad_norm', type=bool, default=True,\n",
    "                    help='norm code gradient from critic->encoder')\n",
    "parser.add_argument('--gan_toenc', type=float, default=-0.01,\n",
    "                    help='weight factor passing gradient from gan to encoder')\n",
    "parser.add_argument('--dropout', type=float, default=0.0,\n",
    "                    help='dropout applied to layers (0 = no dropout)')\n",
    "\n",
    "# Training Arguments\n",
    "parser.add_argument('--epochs', type=int, default=15,\n",
    "                    help='maximum number of epochs')\n",
    "parser.add_argument('--min_epochs', type=int, default=6,\n",
    "                    help=\"minimum number of epochs to train for\")\n",
    "parser.add_argument('--no_earlystopping', action='store_true',\n",
    "                    help=\"won't use KenLM for early stopping\")\n",
    "parser.add_argument('--patience', type=int, default=5,\n",
    "                    help=\"number of language model evaluations without ppl \"\n",
    "                         \"improvement to wait before early stopping\")\n",
    "parser.add_argument('--batch_size', type=int, default=64, metavar='N',\n",
    "                    help='batch size')\n",
    "parser.add_argument('--niters_ae', type=int, default=1,\n",
    "                    help='number of autoencoder iterations in training')\n",
    "parser.add_argument('--niters_gan_d', type=int, default=5,\n",
    "                    help='number of discriminator iterations in training')\n",
    "parser.add_argument('--niters_gan_g', type=int, default=1,\n",
    "                    help='number of generator iterations in training')\n",
    "parser.add_argument('--niters_gan_schedule', type=str, default='2-4-6',\n",
    "                    help='epoch counts to increase number of GAN training '\n",
    "                         ' iterations (increment by 1 each time)')\n",
    "parser.add_argument('--lr_ae', type=float, default=1,\n",
    "                    help='autoencoder learning rate')\n",
    "parser.add_argument('--lr_gan_g', type=float, default=5e-05,\n",
    "                    help='generator learning rate')\n",
    "parser.add_argument('--lr_gan_d', type=float, default=1e-05,\n",
    "                    help='critic/discriminator learning rate')\n",
    "parser.add_argument('--beta1', type=float, default=0.9,\n",
    "                    help='beta1 for adam. default=0.9')\n",
    "parser.add_argument('--clip', type=float, default=1,\n",
    "                    help='gradient clipping, max norm')\n",
    "parser.add_argument('--gan_clamp', type=float, default=0.01,\n",
    "                    help='WGAN clamp')\n",
    "\n",
    "# Evaluation Arguments\n",
    "parser.add_argument('--sample', action='store_true',\n",
    "                    help='sample when decoding for generation')\n",
    "parser.add_argument('--N', type=int, default=5,\n",
    "                    help='N-gram order for training n-gram language model')\n",
    "parser.add_argument('--log_interval', type=int, default=200,\n",
    "                    help='interval to log autoencoder training results')\n",
    "\n",
    "# Other\n",
    "parser.add_argument('--seed', type=int, default=1111,\n",
    "                    help='random seed')\n",
    "parser.add_argument('--cuda', action='store_true',\n",
    "                    help='use CUDA')\n",
    "\n",
    "args = parser.parse_args()\n",
    "print(vars(args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make output directory if it doesn't already exist\n",
    "if not os.path.isdir('./output'):\n",
    "    os.makedirs('./output')\n",
    "if not os.path.isdir('./output/{}'.format(args.outf)):\n",
    "    os.makedirs('./output/{}'.format(args.outf))\n",
    "\n",
    "# Set the random seed manually for reproducibility.\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "if torch.cuda.is_available():\n",
    "    if not args.cuda:\n",
    "        print(\"WARNING: You have a CUDA device, \"\n",
    "              \"so you should probably run with --cuda\")\n",
    "    else:\n",
    "        torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "###############################################################################\n",
    "# Load data\n",
    "###############################################################################\n",
    "\n",
    "# create corpus\n",
    "corpus = Corpus(args.data_path,\n",
    "                maxlen=args.maxlen,\n",
    "                vocab_size=args.vocab_size,\n",
    "                lowercase=args.lowercase)\n",
    "# dumping vocabulary\n",
    "with open('./output/{}/vocab.json'.format(args.outf), 'w') as f:\n",
    "    json.dump(corpus.dictionary.word2idx, f)\n",
    "\n",
    "# save arguments\n",
    "ntokens = len(corpus.dictionary.word2idx)\n",
    "print(\"Vocabulary Size: {}\".format(ntokens))\n",
    "args.ntokens = ntokens\n",
    "with open('./output/{}/args.json'.format(args.outf), 'w') as f:\n",
    "    json.dump(vars(args), f)\n",
    "with open(\"./output/{}/logs.txt\".format(args.outf), 'w') as f:\n",
    "    f.write(str(vars(args)))\n",
    "    f.write(\"\\n\\n\")\n",
    "\n",
    "eval_batch_size = 10\n",
    "test_data = batchify(corpus.test, eval_batch_size, shuffle=False)\n",
    "train_data = batchify(corpus.train, args.batch_size, shuffle=True)\n",
    "\n",
    "print(\"Loaded data!\")\n",
    "\n",
    "###############################################################################\n",
    "# Build the models\n",
    "###############################################################################\n",
    "\n",
    "ntokens = len(corpus.dictionary.word2idx)\n",
    "autoencoder = Seq2Seq(emsize=args.emsize,\n",
    "                      nhidden=args.nhidden,\n",
    "                      ntokens=ntokens,\n",
    "                      nlayers=args.nlayers,\n",
    "                      noise_radius=args.noise_radius,\n",
    "                      hidden_init=args.hidden_init,\n",
    "                      dropout=args.dropout,\n",
    "                      gpu=args.cuda)\n",
    "\n",
    "gan_gen = MLP_G(ninput=args.z_size, noutput=args.nhidden, layers=args.arch_g)\n",
    "gan_disc = MLP_D(ninput=args.nhidden, noutput=1, layers=args.arch_d)\n",
    "\n",
    "print(autoencoder)\n",
    "print(gan_gen)\n",
    "print(gan_disc)\n",
    "\n",
    "optimizer_ae = optim.SGD(autoencoder.parameters(), lr=args.lr_ae)\n",
    "optimizer_gan_g = optim.Adam(gan_gen.parameters(),\n",
    "                             lr=args.lr_gan_g,\n",
    "                             betas=(args.beta1, 0.999))\n",
    "optimizer_gan_d = optim.Adam(gan_disc.parameters(),\n",
    "                             lr=args.lr_gan_d,\n",
    "                             betas=(args.beta1, 0.999))\n",
    "\n",
    "criterion_ce = nn.CrossEntropyLoss()\n",
    "\n",
    "if args.cuda:\n",
    "    autoencoder = autoencoder.cuda()\n",
    "    gan_gen = gan_gen.cuda()\n",
    "    gan_disc = gan_disc.cuda()\n",
    "    criterion_ce = criterion_ce.cuda()\n",
    "\n",
    "###############################################################################\n",
    "# Training code\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "def save_model():\n",
    "    print(\"Saving models\")\n",
    "    with open('./output/{}/autoencoder_model.pt'.format(args.outf), 'wb') as f:\n",
    "        torch.save(autoencoder.state_dict(), f)\n",
    "    with open('./output/{}/gan_gen_model.pt'.format(args.outf), 'wb') as f:\n",
    "        torch.save(gan_gen.state_dict(), f)\n",
    "    with open('./output/{}/gan_disc_model.pt'.format(args.outf), 'wb') as f:\n",
    "        torch.save(gan_disc.state_dict(), f)\n",
    "\n",
    "\n",
    "def evaluate_autoencoder(data_source, epoch):\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    autoencoder.eval()\n",
    "    total_loss = 0\n",
    "    ntokens = len(corpus.dictionary.word2idx)\n",
    "    all_accuracies = 0\n",
    "    bcnt = 0\n",
    "    for i, batch in enumerate(data_source):\n",
    "        source, target, lengths = batch\n",
    "        source = to_gpu(args.cuda, Variable(source, volatile=True))\n",
    "        target = to_gpu(args.cuda, Variable(target, volatile=True))\n",
    "\n",
    "        mask = target.gt(0)\n",
    "        masked_target = target.masked_select(mask)\n",
    "        # examples x ntokens\n",
    "        output_mask = mask.unsqueeze(1).expand(mask.size(0), ntokens)\n",
    "\n",
    "        # output: batch x seq_len x ntokens\n",
    "        output = autoencoder(source, lengths, noise=True)\n",
    "        flattened_output = output.view(-1, ntokens)\n",
    "\n",
    "        masked_output = \\\n",
    "            flattened_output.masked_select(output_mask).view(-1, ntokens)\n",
    "        total_loss += criterion_ce(masked_output/args.temp, masked_target).data\n",
    "\n",
    "        # accuracy\n",
    "        max_vals, max_indices = torch.max(masked_output, 1)\n",
    "        all_accuracies += \\\n",
    "            torch.mean(max_indices.eq(masked_target).float()).data[0]\n",
    "        bcnt += 1\n",
    "\n",
    "        aeoutf = \"./output/%s/%d_autoencoder.txt\" % (args.outf, epoch)\n",
    "        with open(aeoutf, \"a\") as f:\n",
    "            max_values, max_indices = torch.max(output, 2)\n",
    "            max_indices = \\\n",
    "                max_indices.view(output.size(0), -1).data.cpu().numpy()\n",
    "            target = target.view(output.size(0), -1).data.cpu().numpy()\n",
    "            for t, idx in zip(target, max_indices):\n",
    "                # real sentence\n",
    "                chars = \" \".join([corpus.dictionary.idx2word[x] for x in t])\n",
    "                f.write(chars)\n",
    "                f.write(\"\\n\")\n",
    "                # autoencoder output sentence\n",
    "                chars = \" \".join([corpus.dictionary.idx2word[x] for x in idx])\n",
    "                f.write(chars)\n",
    "                f.write(\"\\n\\n\")\n",
    "\n",
    "    return total_loss[0] / len(data_source), all_accuracies/bcnt\n",
    "\n",
    "\n",
    "def evaluate_generator(noise, epoch):\n",
    "    gan_gen.eval()\n",
    "    autoencoder.eval()\n",
    "\n",
    "    # generate from fixed random noise\n",
    "    fake_hidden = gan_gen(noise)\n",
    "    max_indices = \\\n",
    "        autoencoder.generate(fake_hidden, args.maxlen, sample=args.sample)\n",
    "\n",
    "    with open(\"./output/%s/%s_generated.txt\" % (args.outf, epoch), \"w\") as f:\n",
    "        max_indices = max_indices.data.cpu().numpy()\n",
    "        for idx in max_indices:\n",
    "            # generated sentence\n",
    "            words = [corpus.dictionary.idx2word[x] for x in idx]\n",
    "            # truncate sentences to first occurrence of <eos>\n",
    "            truncated_sent = []\n",
    "            for w in words:\n",
    "                if w != '<eos>':\n",
    "                    truncated_sent.append(w)\n",
    "                else:\n",
    "                    break\n",
    "            chars = \" \".join(truncated_sent)\n",
    "            f.write(chars)\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "\n",
    "def train_lm(eval_path, save_path):\n",
    "    # generate examples\n",
    "    indices = []\n",
    "    noise = to_gpu(args.cuda, Variable(torch.ones(100, args.z_size)))\n",
    "    for i in range(1000):\n",
    "        noise.data.normal_(0, 1)\n",
    "\n",
    "        fake_hidden = gan_gen(noise)\n",
    "        max_indices = autoencoder.generate(fake_hidden, args.maxlen)\n",
    "        indices.append(max_indices.data.cpu().numpy())\n",
    "\n",
    "    indices = np.concatenate(indices, axis=0)\n",
    "\n",
    "    # write generated sentences to text file\n",
    "    with open(save_path+\".txt\", \"w\") as f:\n",
    "        # laplacian smoothing\n",
    "        for word in corpus.dictionary.word2idx.keys():\n",
    "            f.write(word+\"\\n\")\n",
    "        for idx in indices:\n",
    "            # generated sentence\n",
    "            words = [corpus.dictionary.idx2word[x] for x in idx]\n",
    "            # truncate sentences to first occurrence of <eos>\n",
    "            truncated_sent = []\n",
    "            for w in words:\n",
    "                if w != '<eos>':\n",
    "                    truncated_sent.append(w)\n",
    "                else:\n",
    "                    break\n",
    "            chars = \" \".join(truncated_sent)\n",
    "            f.write(chars+\"\\n\")\n",
    "\n",
    "    # train language model on generated examples\n",
    "    lm = train_ngram_lm(kenlm_path=args.kenlm_path,\n",
    "                        data_path=save_path+\".txt\",\n",
    "                        output_path=save_path+\".arpa\",\n",
    "                        N=args.N)\n",
    "\n",
    "    # load sentences to evaluate on\n",
    "    with open(eval_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    sentences = [l.replace('\\n', '') for l in lines]\n",
    "    ppl = get_ppl(lm, sentences)\n",
    "\n",
    "    return ppl\n",
    "\n",
    "\n",
    "def train_ae(batch, total_loss_ae, start_time, i):\n",
    "    autoencoder.train()\n",
    "    autoencoder.zero_grad()\n",
    "\n",
    "    source, target, lengths = batch\n",
    "    source = to_gpu(args.cuda, Variable(source))\n",
    "    target = to_gpu(args.cuda, Variable(target))\n",
    "\n",
    "    # Create sentence length mask over padding\n",
    "    mask = target.gt(0)\n",
    "    masked_target = target.masked_select(mask)\n",
    "    # examples x ntokens\n",
    "    output_mask = mask.unsqueeze(1).expand(mask.size(0), ntokens)\n",
    "\n",
    "    # output: batch x seq_len x ntokens\n",
    "    output = autoencoder(source, lengths, noise=True)\n",
    "\n",
    "    # output_size: batch_size, maxlen, self.ntokens\n",
    "    flattened_output = output.view(-1, ntokens)\n",
    "\n",
    "    masked_output = \\\n",
    "        flattened_output.masked_select(output_mask).view(-1, ntokens)\n",
    "    loss = criterion_ce(masked_output/args.temp, masked_target)\n",
    "    loss.backward()\n",
    "\n",
    "    # `clip_grad_norm` to prevent exploding gradient in RNNs / LSTMs\n",
    "    torch.nn.utils.clip_grad_norm(autoencoder.parameters(), args.clip)\n",
    "    optimizer_ae.step()\n",
    "\n",
    "    total_loss_ae += loss.data\n",
    "\n",
    "    accuracy = None\n",
    "    if i % args.log_interval == 0 and i > 0:\n",
    "        # accuracy\n",
    "        probs = F.softmax(masked_output)\n",
    "        max_vals, max_indices = torch.max(probs, 1)\n",
    "        accuracy = torch.mean(max_indices.eq(masked_target).float()).data[0]\n",
    "\n",
    "        cur_loss = total_loss_ae[0] / args.log_interval\n",
    "        elapsed = time.time() - start_time\n",
    "        print('| epoch {:3d} | {:5d}/{:5d} batches | ms/batch {:5.2f} | '\n",
    "              'loss {:5.2f} | ppl {:8.2f} | acc {:8.2f}'\n",
    "              .format(epoch, i, len(train_data),\n",
    "                      elapsed * 1000 / args.log_interval,\n",
    "                      cur_loss, math.exp(cur_loss), accuracy))\n",
    "\n",
    "        with open(\"./output/{}/logs.txt\".format(args.outf), 'a') as f:\n",
    "            f.write('| epoch {:3d} | {:5d}/{:5d} batches | ms/batch {:5.2f} | '\n",
    "                    'loss {:5.2f} | ppl {:8.2f} | acc {:8.2f}\\n'.\n",
    "                    format(epoch, i, len(train_data),\n",
    "                           elapsed * 1000 / args.log_interval,\n",
    "                           cur_loss, math.exp(cur_loss), accuracy))\n",
    "\n",
    "        total_loss_ae = 0\n",
    "        start_time = time.time()\n",
    "\n",
    "    return total_loss_ae, start_time\n",
    "\n",
    "\n",
    "def train_gan_g():\n",
    "    gan_gen.train()\n",
    "    gan_gen.zero_grad()\n",
    "\n",
    "    noise = to_gpu(args.cuda,\n",
    "                   Variable(torch.ones(args.batch_size, args.z_size)))\n",
    "    noise.data.normal_(0, 1)\n",
    "\n",
    "    fake_hidden = gan_gen(noise)\n",
    "    errG = gan_disc(fake_hidden)\n",
    "\n",
    "    # loss / backprop\n",
    "    errG.backward(one)\n",
    "    optimizer_gan_g.step()\n",
    "\n",
    "    return errG\n",
    "\n",
    "\n",
    "def grad_hook(grad):\n",
    "    # Gradient norm: regularize to be same\n",
    "    # code_grad_gan * code_grad_ae / norm(code_grad_gan)\n",
    "    if args.enc_grad_norm:\n",
    "        gan_norm = torch.norm(grad, 2, 1).detach().data.mean()\n",
    "        normed_grad = grad * autoencoder.grad_norm / gan_norm\n",
    "    else:\n",
    "        normed_grad = grad\n",
    "\n",
    "    # weight factor and sign flip\n",
    "    normed_grad *= -math.fabs(args.gan_toenc)\n",
    "    return normed_grad\n",
    "\n",
    "\n",
    "def train_gan_d(batch):\n",
    "    # clamp parameters to a cube\n",
    "    for p in gan_disc.parameters():\n",
    "        p.data.clamp_(-args.gan_clamp, args.gan_clamp)\n",
    "\n",
    "    autoencoder.train()\n",
    "    autoencoder.zero_grad()\n",
    "    gan_disc.train()\n",
    "    gan_disc.zero_grad()\n",
    "\n",
    "    # positive samples ----------------------------\n",
    "    # generate real codes\n",
    "    source, target, lengths = batch\n",
    "    source = to_gpu(args.cuda, Variable(source))\n",
    "    target = to_gpu(args.cuda, Variable(target))\n",
    "\n",
    "    # batch_size x nhidden\n",
    "    real_hidden = autoencoder(source, lengths, noise=False, encode_only=True)\n",
    "    real_hidden.register_hook(grad_hook)\n",
    "\n",
    "    # loss / backprop\n",
    "    errD_real = gan_disc(real_hidden)\n",
    "    errD_real.backward(one)\n",
    "\n",
    "    # negative samples ----------------------------\n",
    "    # generate fake codes\n",
    "    noise = to_gpu(args.cuda,\n",
    "                   Variable(torch.ones(args.batch_size, args.z_size)))\n",
    "    noise.data.normal_(0, 1)\n",
    "\n",
    "    # loss / backprop\n",
    "    fake_hidden = gan_gen(noise)\n",
    "    errD_fake = gan_disc(fake_hidden.detach())\n",
    "    errD_fake.backward(mone)\n",
    "\n",
    "    # `clip_grad_norm` to prvent exploding gradient problem in RNNs / LSTMs\n",
    "    torch.nn.utils.clip_grad_norm(autoencoder.parameters(), args.clip)\n",
    "\n",
    "    optimizer_gan_d.step()\n",
    "    optimizer_ae.step()\n",
    "    errD = -(errD_real - errD_fake)\n",
    "\n",
    "    return errD, errD_real, errD_fake\n",
    "\n",
    "\n",
    "print(\"Training...\")\n",
    "with open(\"./output/{}/logs.txt\".format(args.outf), 'a') as f:\n",
    "    f.write('Training...\\n')\n",
    "\n",
    "# schedule of increasing GAN training loops\n",
    "if args.niters_gan_schedule != \"\":\n",
    "    gan_schedule = [int(x) for x in args.niters_gan_schedule.split(\"-\")]\n",
    "else:\n",
    "    gan_schedule = []\n",
    "niter_gan = 1\n",
    "\n",
    "fixed_noise = to_gpu(args.cuda,\n",
    "                     Variable(torch.ones(args.batch_size, args.z_size)))\n",
    "fixed_noise.data.normal_(0, 1)\n",
    "one = to_gpu(args.cuda, torch.FloatTensor([1]))\n",
    "mone = one * -1\n",
    "\n",
    "best_ppl = None\n",
    "impatience = 0\n",
    "all_ppl = []\n",
    "for epoch in range(1, args.epochs+1):\n",
    "    # update gan training schedule\n",
    "    if epoch in gan_schedule:\n",
    "        niter_gan += 1\n",
    "        print(\"GAN training loop schedule increased to {}\".format(niter_gan))\n",
    "        with open(\"./output/{}/logs.txt\".format(args.outf), 'a') as f:\n",
    "            f.write(\"GAN training loop schedule increased to {}\\n\".\n",
    "                    format(niter_gan))\n",
    "\n",
    "    total_loss_ae = 0\n",
    "    epoch_start_time = time.time()\n",
    "    start_time = time.time()\n",
    "    niter = 0\n",
    "    niter_global = 1\n",
    "\n",
    "    # loop through all batches in training data\n",
    "    while niter < len(train_data):\n",
    "\n",
    "        # train autoencoder ----------------------------\n",
    "        for i in range(args.niters_ae):\n",
    "            if niter == len(train_data):\n",
    "                break  # end of epoch\n",
    "            total_loss_ae, start_time = \\\n",
    "                train_ae(train_data[niter], total_loss_ae, start_time, niter)\n",
    "            niter += 1\n",
    "\n",
    "        # train gan ----------------------------------\n",
    "        for k in range(niter_gan):\n",
    "\n",
    "            # train discriminator/critic\n",
    "            for i in range(args.niters_gan_d):\n",
    "                # feed a seen sample within this epoch; good for early training\n",
    "                errD, errD_real, errD_fake = \\\n",
    "                    train_gan_d(train_data[random.randint(0, len(train_data)-1)])\n",
    "\n",
    "            # train generator\n",
    "            for i in range(args.niters_gan_g):\n",
    "                errG = train_gan_g()\n",
    "\n",
    "        niter_global += 1\n",
    "        if niter_global % 100 == 0:\n",
    "            print('[%d/%d][%d/%d] Loss_D: %.8f (Loss_D_real: %.8f '\n",
    "                  'Loss_D_fake: %.8f) Loss_G: %.8f'\n",
    "                  % (epoch, args.epochs, niter, len(train_data),\n",
    "                     errD.data[0], errD_real.data[0],\n",
    "                     errD_fake.data[0], errG.data[0]))\n",
    "            with open(\"./output/{}/logs.txt\".format(args.outf), 'a') as f:\n",
    "                f.write('[%d/%d][%d/%d] Loss_D: %.8f (Loss_D_real: %.8f '\n",
    "                        'Loss_D_fake: %.8f) Loss_G: %.8f\\n'\n",
    "                        % (epoch, args.epochs, niter, len(train_data),\n",
    "                           errD.data[0], errD_real.data[0],\n",
    "                           errD_fake.data[0], errG.data[0]))\n",
    "\n",
    "            # exponentially decaying noise on autoencoder\n",
    "            autoencoder.noise_radius = \\\n",
    "                autoencoder.noise_radius*args.noise_anneal\n",
    "\n",
    "            if niter_global % 3000 == 0:\n",
    "                evaluate_generator(fixed_noise, \"epoch{}_step{}\".\n",
    "                                   format(epoch, niter_global))\n",
    "\n",
    "                # evaluate with lm\n",
    "                if not args.no_earlystopping and epoch > args.min_epochs:\n",
    "                    ppl = train_lm(eval_path=os.path.join(args.data_path,\n",
    "                                                          \"test.txt\"),\n",
    "                                   save_path=\"output/{}/\"\n",
    "                                             \"epoch{}_step{}_lm_generations\".\n",
    "                                             format(args.outf, epoch,\n",
    "                                                    niter_global))\n",
    "                    print(\"Perplexity {}\".format(ppl))\n",
    "                    all_ppl.append(ppl)\n",
    "                    print(all_ppl)\n",
    "                    with open(\"./output/{}/logs.txt\".\n",
    "                              format(args.outf), 'a') as f:\n",
    "                        f.write(\"\\n\\nPerplexity {}\\n\".format(ppl))\n",
    "                        f.write(str(all_ppl)+\"\\n\\n\")\n",
    "                    if best_ppl is None or ppl < best_ppl:\n",
    "                        impatience = 0\n",
    "                        best_ppl = ppl\n",
    "                        print(\"New best ppl {}\\n\".format(best_ppl))\n",
    "                        with open(\"./output/{}/logs.txt\".\n",
    "                                  format(args.outf), 'a') as f:\n",
    "                            f.write(\"New best ppl {}\\n\".format(best_ppl))\n",
    "                        save_model()\n",
    "                    else:\n",
    "                        impatience += 1\n",
    "                        # end training\n",
    "                        if impatience > args.patience:\n",
    "                            print(\"Ending training\")\n",
    "                            with open(\"./output/{}/logs.txt\".\n",
    "                                      format(args.outf), 'a') as f:\n",
    "                                f.write(\"\\nEnding Training\\n\")\n",
    "                            sys.exit()\n",
    "\n",
    "    # end of epoch ----------------------------\n",
    "    # evaluation\n",
    "    test_loss, accuracy = evaluate_autoencoder(test_data, epoch)\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | test loss {:5.2f} | '\n",
    "          'test ppl {:5.2f} | acc {:3.3f}'.\n",
    "          format(epoch, (time.time() - epoch_start_time),\n",
    "                 test_loss, math.exp(test_loss), accuracy))\n",
    "    print('-' * 89)\n",
    "\n",
    "    with open(\"./output/{}/logs.txt\".format(args.outf), 'a') as f:\n",
    "        f.write('-' * 89)\n",
    "        f.write('\\n| end of epoch {:3d} | time: {:5.2f}s | test loss {:5.2f} |'\n",
    "                ' test ppl {:5.2f} | acc {:3.3f}\\n'.\n",
    "                format(epoch, (time.time() - epoch_start_time),\n",
    "                       test_loss, math.exp(test_loss), accuracy))\n",
    "        f.write('-' * 89)\n",
    "        f.write('\\n')\n",
    "\n",
    "    evaluate_generator(fixed_noise, \"end_of_epoch_{}\".format(epoch))\n",
    "    if not args.no_earlystopping and epoch >= args.min_epochs:\n",
    "        ppl = train_lm(eval_path=os.path.join(args.data_path, \"test.txt\"),\n",
    "                       save_path=\"./output/{}/end_of_epoch{}_lm_generations\".\n",
    "                                 format(args.outf, epoch))\n",
    "        print(\"Perplexity {}\".format(ppl))\n",
    "        all_ppl.append(ppl)\n",
    "        print(all_ppl)\n",
    "        with open(\"./output/{}/logs.txt\".format(args.outf), 'a') as f:\n",
    "            f.write(\"\\n\\nPerplexity {}\\n\".format(ppl))\n",
    "            f.write(str(all_ppl)+\"\\n\\n\")\n",
    "        if best_ppl is None or ppl < best_ppl:\n",
    "            impatience = 0\n",
    "            best_ppl = ppl\n",
    "            print(\"New best ppl {}\\n\".format(best_ppl))\n",
    "            with open(\"./output/{}/logs.txt\".format(args.outf), 'a') as f:\n",
    "                f.write(\"New best ppl {}\\n\".format(best_ppl))\n",
    "            save_model()\n",
    "        else:\n",
    "            impatience += 1\n",
    "            # end training\n",
    "            if impatience > args.patience:\n",
    "                print(\"Ending training\")\n",
    "                with open(\"./output/{}/logs.txt\".format(args.outf), 'a') as f:\n",
    "                    f.write(\"\\nEnding Training\\n\")\n",
    "                sys.exit()\n",
    "\n",
    "    # shuffle between epochs\n",
    "    train_data = batchify(corpus.train, args.batch_size, shuffle=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
